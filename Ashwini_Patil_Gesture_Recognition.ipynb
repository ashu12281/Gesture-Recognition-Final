{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtN1NBZd4rzudAJrpFjKew"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS5ftLNcsXtL",
        "outputId": "6eef1a18-e274-40d2-f1ff-3ce3065ebc1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 20 16:07:49 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "#from scipy.misc import imread, imresize\n",
        "import datetime\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import abc\n",
        "from sys import getsizeof"
      ],
      "metadata": {
        "id": "nx4PSBF2tBul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting the random seed so that the results don't vary drastically."
      ],
      "metadata": {
        "id": "7aDdRIamuKDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.set_random_seed(30)"
      ],
      "metadata": {
        "id": "gga5zZp-uMD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D , Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.applications import mobilenet\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau ,EarlyStopping\n",
        "from keras import optimizers"
      ],
      "metadata": {
        "id": "eMRmxvO3ucls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##mount the drive to get the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyD5Mj9eu1PW",
        "outputId": "b3f28f3c-2e60-4721-e0c0-a2a3925b6cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Getting the dataset to the colab runtime local memory\n",
        "!pip install -U -q PyDrive"
      ],
      "metadata": {
        "id": "XFdiJP9ovJZM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "8ga2qCRsvyet"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '1ehyrYBQ5rbQQe6yL4XbLWe3FMvuVUGiL' # URL id.\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('Project_data.zip')"
      ],
      "metadata": {
        "id": "IWf0dTrMwA2F"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Project_data.zip"
      ],
      "metadata": {
        "id": "E79V1BXZwcFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the project folder path\n",
        "project_folder_path = \"/content/Project_data\""
      ],
      "metadata": {
        "id": "VcPpM_D9wk22"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ModelClass:\n",
        "\n",
        "This model class will handle all the initialisations of paths, image properties and model properties\n",
        "\n",
        "This class will have a generator function as well for generating the data with labels\n",
        "\n",
        "The images will be cropped to the size we want\n",
        "\n",
        "The images with irregular dimension will be center cropped on the x-axis equally on both sides\n",
        "\n",
        "We save the model only when the validation loss decreases\n",
        "\n",
        "Learning rate decreases when approaching the minima\n",
        "\n",
        "This class will have an abstract method to define our own model by inheriting this class\n",
        "\n",
        "This class will have a generic train function as well.\n",
        "\n",
        "The history is saved to plot the model as well"
      ],
      "metadata": {
        "id": "lCa_bUJ2xGqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelClass(metaclass= abc.ABCMeta):\n",
        "    # initialisng the path where project data resides\n",
        "    def initialize_path(self,project_folder_path):\n",
        "        self.train_doc = np.random.permutation(open(project_folder_path + '/' + 'train.csv').readlines())\n",
        "        self.val_doc = np.random.permutation(open(project_folder_path + '/' + 'val.csv').readlines())\n",
        "        self.train_path = project_folder_path + '/' + 'train'\n",
        "        self.val_path =  project_folder_path + '/' + 'val'\n",
        "        self.num_train_sequences = len(self.train_doc)\n",
        "        self.num_val_sequences = len(self.val_doc)\n",
        "\n",
        "    # initialising the image properties\n",
        "    def initialize_image_properties(self,image_height=120,image_width=120):\n",
        "        self.image_height=image_height\n",
        "        self.image_width=image_width\n",
        "        self.channels=3\n",
        "        self.num_classes=5\n",
        "        self.total_frames=30\n",
        "\n",
        "    # initialising the batch size, frames to sample and the no. of epochs\n",
        "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=32,num_epochs=50):\n",
        "        self.frames_to_sample=frames_to_sample\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "\n",
        "    def initialize_modelparams(self):\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def generator(self,source_path, folder_list, model_type = \"Conv3D\"):\n",
        "      if model_type == \"Conv3D\":\n",
        "        img_idx = [rn.randint(0, self.total_frames-1) for i in range(self.frames_to_sample)] #create a list of image numbers you want to use for a particular video\n",
        "      else:\n",
        "        img_idx = [i for i in range(self.frames_to_sample)] ## CNN+GRU\n",
        "      # print(img_idx)\n",
        "      while True:\n",
        "          t = np.random.permutation(folder_list)\n",
        "          num_batches = len(folder_list)//self.batch_size # calculate the number of batches\n",
        "          for batch_no in range(num_batches): # we iterate over the number of batches\n",
        "              batch_data = np.zeros((self.batch_size,len(img_idx),self.image_height,self.image_width,self.channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "              batch_labels = np.zeros((self.batch_size,self.num_classes)) # batch_labels is the one hot representation of the output\n",
        "              for folder in range(self.batch_size): # iterate over the batch_size\n",
        "                  imgs = os.listdir(source_path+'/'+ t[folder + (batch_no*self.batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                  # print(\"imgs: \",imgs)\n",
        "                  for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                      image = imread(source_path+'/'+ t[folder + (batch_no*self.batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                      if image.shape == (360,360,3): ###if images are of 360 x 360\n",
        "                        image = imresize(image,(self.image_height,self.image_width,self.channels))\n",
        "                      else:\n",
        "                        image = image[:,(image.shape[0] - self.image_height ) // 2 : image.shape[0] - (image.shape[0] - self.image_height ) // 2] ### if lower quality just centre crop\n",
        "                      #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                      #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "\n",
        "\n",
        "                      batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #normalise and feed in the image\n",
        "                      batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0 #normalise and feed in the image\n",
        "                      batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0 #normalise and feed in the image\n",
        "\n",
        "                  batch_labels[folder, int(t[folder + (batch_no*self.batch_size)].strip().split(';')[2])] = 1\n",
        "              yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "\n",
        "      #     write the code for the remaining data points which are left after full batches\n",
        "          left_images = len(folder_list)%self.batch_size\n",
        "          left_overs = t[len(t)-left_images:]\n",
        "\n",
        "          batch_data = np.zeros((left_images,len(img_idx),self.image_height,self.image_width,self.channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "          batch_labels = np.zeros((left_images,self.num_classes)) # batch_labels is the one hot representation of the output\n",
        "          for folder in range(left_images): # iterate over the batch_size\n",
        "              imgs = os.listdir(source_path+'/'+ left_overs[folder].split(';')[0]) # read all the images in the folder\n",
        "              # print(\"imgs: \",imgs)\n",
        "              for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                  image = imread(source_path+'/'+ left_overs[folder].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                  if image.shape == (360,360,3): ###if images are of 360 x 360\n",
        "                        image = imresize(image,(self.image_height,self.image_width,self.channels))\n",
        "                  else:\n",
        "                    image = image[:,(image.shape[0] - self.image_height ) // 2 : image.shape[0] - (image.shape[0] - self.image_height ) // 2] ### if lower quality just centre crop\n",
        "                  #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                  #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "\n",
        "\n",
        "                  batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #normalise and feed in the image\n",
        "                  batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0 #normalise and feed in the image\n",
        "                  batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0 #normalise and feed in the image\n",
        "\n",
        "              batch_labels[folder, int(left_overs[folder].strip().split(';')[2])] = 1\n",
        "          yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def model_architecture(self):\n",
        "      pass\n",
        "\n",
        "    def train_model(self,model,model_name=\"model_init\",model_type=\"Conv3D\"):\n",
        "        train_generator = self.generator(self.train_path, self.train_doc,model_type=model_type)\n",
        "        val_generator = self.generator(self.val_path, self.val_doc,model_type=model_type)\n",
        "\n",
        "        model_name = model_name + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "        if not os.path.exists(model_name):\n",
        "            os.mkdir(model_name)\n",
        "\n",
        "        filepath = \"/content/drive/MyDrive/\"+model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
        "\n",
        "        # earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n",
        "        callbacks_list = [checkpoint, LR]\n",
        "\n",
        "        if (self.num_train_sequences%self.batch_size) == 0:\n",
        "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
        "        else:\n",
        "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
        "\n",
        "        if (self.num_val_sequences%self.batch_size) == 0:\n",
        "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
        "        else:\n",
        "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
        "\n",
        "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1,\n",
        "                            callbacks=callbacks_list, validation_data=val_generator,\n",
        "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
        "        self.history = history\n",
        "        return history\n",
        "\n",
        "    def plot_model(self):\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
        "        axes[0].plot(self.history.history['loss'])\n",
        "        axes[0].plot(self.history.history['val_loss'])\n",
        "        axes[0].legend(['loss','val_loss'])\n",
        "\n",
        "        axes[1].plot(self.history.history['categorical_accuracy'])\n",
        "        axes[1].plot(self.history.history['val_categorical_accuracy'])\n",
        "        axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
      ],
      "metadata": {
        "id": "aDb992xgxQK0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEts check sample image cropping"
      ],
      "metadata": {
        "id": "qYbcEP6Sx9Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "#from skimage.io import imread, imresize"
      ],
      "metadata": {
        "id": "fkj8su8EyZP9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##actual image\n",
        "image = imageio.imread('/content/drive/MyDrive/Project_data/train/WIN_20180925_17_44_57_Pro_Thumbs_Down_new/WIN_20180925_17_44_57_Pro_00015.png')\n",
        "print(image.shape)\n",
        "plt.imshow(image/255)"
      ],
      "metadata": {
        "id": "Y7F5sP0tyAgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##centre cropping of the image looks like this for a (120,160) image\n",
        "image = image[:,20:140]\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iCjP46815NUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Building Approach 1: Conv3D model**\n",
        "\n",
        "Lets write base model which has Conv3D + BatchNorm + MaxPooling3D layers lined up with a softmax at the end"
      ],
      "metadata": {
        "id": "e4Y3rcDn5SFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv3D1(ModelClass):\n",
        "\n",
        "    def model_architecture(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = \"adam\"\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "metadata": {
        "id": "YqiSOaoi5OXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: Conv3D model**\n",
        "\n",
        "with\n",
        "\n",
        "just 15 frames to be sampled\n",
        "\n",
        "batch size of 32\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers"
      ],
      "metadata": {
        "id": "GghiPDQp5iBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D1 = Conv3D1()\n",
        "conv3D1.initialize_path(project_folder_path)\n",
        "conv3D1.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv3D1.initialize_hyperparams(frames_to_sample=15,batch_size=32,num_epochs=20)\n",
        "conv3D1.initialize_modelparams()\n",
        "conv3D1_model=conv3D1.model_architecture(dense_neurons=128,dropout=0.25)\n",
        "conv3D1_model.summary()"
      ],
      "metadata": {
        "id": "Ef8A7B1l5svu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##train the model\n",
        "conv3D1.train_model(conv3D1_model,model_name=\"conv3D1\",model_type=\"Conv3D\")"
      ],
      "metadata": {
        "id": "rudDHQjM51J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the model\n",
        "conv3D1.plot_model()"
      ],
      "metadata": {
        "id": "4Vbvrn-R55El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "Clearly overfitting with val accuracy just being around 30% and train accuracy reaching 99%+\n",
        "\n",
        "Let's decrease the batch_size to reduce generalisability and increase the epochs and also increase the frames to sample\n",
        "\n",
        "**Model 2: Conv3D model** with\n",
        "\n",
        "20 frames to be sampled\n",
        "\n",
        "batch size of 20\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers\n",
        "\n",
        "Epochs = 30"
      ],
      "metadata": {
        "id": "TjwoSCQg5-J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D2 = Conv3D1()\n",
        "conv3D2.initialize_path(project_folder_path)\n",
        "conv3D2.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv3D2.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
        "conv3D2.initialize_modelparams()\n",
        "conv3D2_model=conv3D2.model_architecture(dense_neurons=128,dropout=0.25)\n",
        "conv3D2_model.summary()"
      ],
      "metadata": {
        "id": "KlKBwoqp6Pqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##train the model\n",
        "conv3D2.train_model(conv3D2_model,model_name=\"conv3D2\",model_type=\"Conv3D\")"
      ],
      "metadata": {
        "id": "FAZWC7726T6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D2.plot_model()"
      ],
      "metadata": {
        "id": "q6hmOxJI6XqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "we have pushed the validation accuracy by a large extent from 20% to 83%\n",
        "\n",
        "We still see there is some gap between training and testing accuracy which is around 15% where training accuracy is 99%\n",
        "\n",
        "We should try increasing dropout and decrease batch_size to see if it reduces the gap\n",
        "\n",
        "Still this model is decent after 19th epoch with loss: 0.0832 - categorical_accuracy: 0.9925 - val_loss: 0.5237 - val_categorical_accuracy: 0.8300. The losses are below 1\n",
        "\n",
        "**Model 3: Conv3D model** with\n",
        "\n",
        "20 frames to be sampled\n",
        "\n",
        "batch size of 16 (perviously 20)\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers\n",
        "\n",
        "Epochs = 30\n",
        "\n",
        "dropout = 0.5"
      ],
      "metadata": {
        "id": "OYSe3AXY6bhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D3 = Conv3D1()\n",
        "conv3D3.initialize_path(project_folder_path)\n",
        "conv3D3.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv3D3.initialize_hyperparams(frames_to_sample=20,batch_size=16,num_epochs=30)\n",
        "conv3D3.initialize_modelparams()\n",
        "conv3D3_model=conv3D3.model_architecture(dense_neurons=128,dropout=0.5)\n",
        "conv3D3_model.summary()"
      ],
      "metadata": {
        "id": "TOep0bu56m_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "conv3D3.train_model(conv3D3_model,model_name=\"conv3D3\",model_type=\"Conv3D\")"
      ],
      "metadata": {
        "id": "qxpbuFu26sQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D3.plot_model()"
      ],
      "metadata": {
        "id": "a4DdgLNy6vrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "We have now reduced the gap between training and validation accuracy, thereby completely overcoming overfitting.\n",
        "\n",
        "We see that least validation loss is at 0.4635\n",
        "\n",
        "The post 25th epoch had loss: 0.4083 - categorical_accuracy: 0.8537 - val_loss: 0.4636 - val_categorical_accuracy: 0.8600\n",
        "\n",
        "We see that the training accuracy is lesser than the validation accuracy.\n",
        "This happens when the validation set is easier to interpret than the training set.\n",
        "\n",
        "This is NOT a negative sign and is much realistic as you see that the training and validation loss are very close by similar to categorical accuracies.\n",
        "Let's try a few more models like above using different kernel size to see if we see some more improvement\n",
        "\n",
        "**Model 4: Conv3D model** with\n",
        "\n",
        "15 frames to be sampled\n",
        "\n",
        "batch size of 32\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers\n",
        "\n",
        "Epochs = 20\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "kernel size = (3,3,3)"
      ],
      "metadata": {
        "id": "keb_K3cf60jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv3D4(ModelClass):\n",
        "\n",
        "    def model_architecture(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv3D(128, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = \"adam\"\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "metadata": {
        "id": "j_z6OYqe6zEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D4 = Conv3D4()\n",
        "conv3D4.initialize_path(project_folder_path)\n",
        "conv3D4.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv3D4.initialize_hyperparams(frames_to_sample=15,batch_size=32,num_epochs=20)\n",
        "conv3D4.initialize_modelparams()\n",
        "conv3D4_model=conv3D4.model_architecture(dense_neurons=128,dropout=0.25)\n",
        "conv3D4_model.summary()"
      ],
      "metadata": {
        "id": "q5sB5lB37HVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D4.train_model(conv3D4_model,model_name=\"conv3D4\",model_type=\"Conv3D\")"
      ],
      "metadata": {
        "id": "pHhBo7C97L1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D4.plot_model()"
      ],
      "metadata": {
        "id": "QTwY2dmP7QGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "A case of complete overfitting is seen here.\n",
        "Lets reduce the batch size\n",
        "\n",
        "**Model 5: Conv3D model** with\n",
        "\n",
        "15 frames to be sampled\n",
        "\n",
        "batch size of 20\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers\n",
        "\n",
        "Epochs = 20\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "kernel size = (3,3,3)"
      ],
      "metadata": {
        "id": "NN-eUPqJ7T6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D5 = Conv3D4()\n",
        "conv3D5.initialize_path(project_folder_path)\n",
        "conv3D5.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv3D5.initialize_hyperparams(frames_to_sample=15,batch_size=20,num_epochs=20)\n",
        "conv3D5.initialize_modelparams()\n",
        "conv3D5_model=conv3D5.model_architecture(dense_neurons=128,dropout=0.25)\n",
        "conv3D5_model.summary()"
      ],
      "metadata": {
        "id": "zuEz7Z1p7Q7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D5.train_model(conv3D5_model,model_name=\"conv3D5\",model_type=\"Conv3D\")"
      ],
      "metadata": {
        "id": "Qq4RpgGw7k2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D5.plot_model()"
      ],
      "metadata": {
        "id": "zPXlyTrT7okt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "We have now reduced the gap between training and validation accuracy, thereby reduced overfitting by a large extent.\n",
        "\n",
        "The validation loss is below 1 (~= 0.86)\n",
        "The model has loss: 0.4693 - categorical_accuracy: 0.8281 - val_loss: 0.8504 - val_categorical_accuracy: 0.7100\n",
        "\n",
        "The accuracies are a lot better than the previous model. Based on the graphs, lets try increasing the epochs, may be we might acheive something better.\n",
        "\n",
        "**Model 6: Conv3D model** with\n",
        "\n",
        "15 frames to be sampled\n",
        "\n",
        "batch size of 20\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers\n",
        "\n",
        "Epochs = 40\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "kernel size = (3,3,3)"
      ],
      "metadata": {
        "id": "CnCOyz1Y7rkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D6 = Conv3D4()\n",
        "conv3D6.initialize_path(project_folder_path)\n",
        "conv3D6.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv3D6.initialize_hyperparams(frames_to_sample=15,batch_size=20,num_epochs=40)\n",
        "conv3D6.initialize_modelparams()\n",
        "conv3D6_model=conv3D6.model_architecture(dense_neurons=128,dropout=0.25)\n",
        "conv3D6_model.summary()"
      ],
      "metadata": {
        "id": "_FJ_kyBp8FKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D6.train_model(conv3D6_model,model_name=\"conv3D6\",model_type=\"Conv3D\")"
      ],
      "metadata": {
        "id": "Z7NbG6_W8GX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D6.plot_model()"
      ],
      "metadata": {
        "id": "ip9-1c6X8JPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "There is improvement in validation accuracy by 2% from 71% to 73%\n",
        "Validation loss improved from 0.86 to 0.79\n",
        "\n",
        "**Model 7: Conv3D model with added convolutional layers**\n",
        "\n",
        "15 frames to be sampled\n",
        "\n",
        "batch size of 32\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers\n",
        "\n",
        "Epochs = 20\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "kernel size = (2,2,2)"
      ],
      "metadata": {
        "id": "hXiixSSq8O5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv3D7(ModelClass):\n",
        "\n",
        "    def model_architecture(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
        "\n",
        "        model.add(Conv3D(256, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
        "\n",
        "        model.add(Conv3D(512, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = \"adam\"\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "metadata": {
        "id": "dVePfL5e8Xn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D7 = Conv3D7()\n",
        "conv3D7.initialize_path(project_folder_path)\n",
        "conv3D7.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv3D7.initialize_hyperparams(frames_to_sample=15,batch_size=32,num_epochs=20)\n",
        "conv3D7.initialize_modelparams()\n",
        "conv3D7_model=conv3D7.model_architecture(dense_neurons=128,dropout=0.25)\n",
        "conv3D7_model.summary()"
      ],
      "metadata": {
        "id": "gCoR4PF88c2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##train the model\n",
        "conv3D7.train_model(conv3D7_model,model_name=\"conv3D7\",model_type=\"Conv3D\")"
      ],
      "metadata": {
        "id": "T-O2XLEj8fK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the model\n",
        "conv3D7.plot_model()"
      ],
      "metadata": {
        "id": "hnctuSht8h-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "Clearly overfitting with val accuracy just being around 24% and train accuracy reaching 97%+\n",
        "\n",
        "Let's decrease the batch_size to increase generalisability\n",
        "\n",
        "**Model 8: Conv3D model with added convolutional layers**\n",
        "\n",
        "15 frames to be sampled\n",
        "\n",
        "batch size of 20\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers\n",
        "\n",
        "Epochs = 20\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "kernel size = (2,2,2)"
      ],
      "metadata": {
        "id": "I7Upws6h8oK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D8 = Conv3D7()\n",
        "conv3D8.initialize_path(project_folder_path)\n",
        "conv3D8.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv3D8.initialize_hyperparams(frames_to_sample=15,batch_size=20,num_epochs=20)\n",
        "conv3D8.initialize_modelparams()\n",
        "conv3D8_model=conv3D8.model_architecture(dense_neurons=128,dropout=0.25)\n",
        "conv3D8_model.summary()"
      ],
      "metadata": {
        "id": "p7Oh7wVM8k71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##train the model\n",
        "conv3D8.train_model(conv3D8_model,model_name=\"conv3D8\",model_type=\"Conv3D\")"
      ],
      "metadata": {
        "id": "TDPzIp6l8x0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D8.plot_model()"
      ],
      "metadata": {
        "id": "-DmVE52x80_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "Validation accuracy improved from 24% to 52%. Training accuracy decreased from 97% to 79%\n",
        "\n",
        "Let's increase the epochs to see if validation increases further\n",
        "\n",
        "**Model 9: Conv3D model with added convolutional layers and 40 epochs**\n",
        "\n",
        "15 frames to be sampled\n",
        "\n",
        "batch size of 20\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers\n",
        "\n",
        "Epochs = 40\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "kernel size = (2,2,2)"
      ],
      "metadata": {
        "id": "ORT6Z-g286tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D9 = Conv3D7()\n",
        "conv3D9.initialize_path(project_folder_path)\n",
        "conv3D9.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv3D9.initialize_hyperparams(frames_to_sample=15,batch_size=20,num_epochs=40)\n",
        "conv3D9.initialize_modelparams()\n",
        "conv3D9_model=conv3D9.model_architecture(dense_neurons=128,dropout=0.25)\n",
        "conv3D9_model.summary()"
      ],
      "metadata": {
        "id": "R_k2s0T283x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##train the model\n",
        "conv3D9.train_model(conv3D9_model,model_name=\"conv3D9\",model_type=\"Conv3D\")"
      ],
      "metadata": {
        "id": "kHA_x9eg9EH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv3D9.plot_model()"
      ],
      "metadata": {
        "id": "siw_7KAt9Ho1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "Validation accuracy improved from 52% to 67%. Training accuracy increased from 79% to 84%\n",
        "\n",
        "We have better model with us compared to this one. Let's finally use the weights of model3 with training accuracy 85% and validation accuracy 86%\n",
        "\n",
        "**Approach 2: CNN+GRU Models**\n",
        "\n",
        "CNN will act as the feature extractor\n",
        "\n",
        "GRU will help us with the TimeSeries prediction of the images\n",
        "\n",
        "ConvGRU class is extension of ModelClass with a different architecture CNN+GRU\n",
        "\n",
        "Base Model with few conv2D- BatchNorm - maxPool ordered layers\n",
        "\n",
        "Using GRU instead of LSTM to avoid more parameters, as we want to deploy on mobile applications and smart TV.\n",
        "\n",
        "**Model 10: Conv2D+GRU**\n",
        "\n",
        "18 frames to be sampled\n",
        "\n",
        "batch size of 20\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons in the two dense layers\n",
        "\n",
        "Epochs = 20\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "gru cells = 128"
      ],
      "metadata": {
        "id": "lFq7QhEP9P-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvGRU(ModelClass):\n",
        "\n",
        "    def model_architecture(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
        "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "\n",
        "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "\n",
        "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "\n",
        "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "\n",
        "        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "\n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "\n",
        "        model.add(GRU(gru_cells))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "        optimiser = \"adam\"\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        self.model = model\n",
        "        return model"
      ],
      "metadata": {
        "id": "9RRDrdZ-9fvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru = ConvGRU()\n",
        "conv_gru.initialize_path(project_folder_path)\n",
        "conv_gru.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_gru.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
        "conv_gru.initialize_modelparams()\n",
        "conv_gru_model=conv_gru.model_architecture(gru_cells=128,dense_neurons=128,dropout=0.25)\n",
        "conv_gru_model.summary()"
      ],
      "metadata": {
        "id": "_Uk7gKDy9jm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru.train_model(conv_gru_model,model_name=\"conv_gru\",model_type=\"CNN_GRU\")\n",
        "conv_gru.plot_model()"
      ],
      "metadata": {
        "id": "ald_DZVe9nwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "We see loss: 0.1611 - categorical_accuracy: 0.9593 - val_loss: 0.7751 - val_categorical_accuracy: 0.7100\n",
        "\n",
        "We see a training loss and validation loss less than 1\n",
        "\n",
        "Accuracies are 95% and 71% which is a sign of overfitting\n",
        "\n",
        "Lets increase dropout and increase batch size to see if it reduces overfitting.\n",
        "\n",
        "**Model 11: Conv2D+GRU**\n",
        "\n",
        "20 frames to be sampled\n",
        "\n",
        "batch size of 32 from 20\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons\n",
        "\n",
        "Epochs = 25\n",
        "\n",
        "dropout = 0.5\n",
        "\n",
        "gru cells = 128"
      ],
      "metadata": {
        "id": "bSj17vmy9y5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru2 = ConvGRU()\n",
        "conv_gru2.initialize_path(project_folder_path)\n",
        "conv_gru2.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_gru2.initialize_hyperparams(frames_to_sample=20,batch_size=32,num_epochs=25)\n",
        "conv_gru2.initialize_modelparams()\n",
        "conv_gru_model2=conv_gru2.model_architecture(gru_cells=128,dense_neurons=128,dropout=0.5)\n",
        "conv_gru_model2.summary()"
      ],
      "metadata": {
        "id": "4a5fBYIb9-8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru2.train_model(conv_gru_model2,model_name=\"conv_gru2\",model_type=\"CNN_GRU\")\n",
        "conv_gru2.plot_model()"
      ],
      "metadata": {
        "id": "K_Fa10K0-GhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "Increasing the batch size didnot help and also increased the overfitting by a large extent\n",
        "\n",
        "The models accuracy on validation decreased from 71 to 43%.\n",
        "\n",
        "Instead of working on scratch models here, lets kick in with transfer learning for better image representation.\n",
        "\n",
        "Lets now try to use transfer learning with MobileNet which is known to be the most prominent set of weights for light weight applications\n",
        "\n",
        "**Model 12: MobileNet+GRU**\n",
        "\n",
        "20 frames to be sampled\n",
        "\n",
        "batch size of 32\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons\n",
        "\n",
        "Epochs = 25\n",
        "\n",
        "dropout = 0.5\n",
        "\n",
        "gru cells = 128"
      ],
      "metadata": {
        "id": "4aEChliW-Mgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
        "\n",
        "class ConvGRU_Transfer(ModelClass):\n",
        "\n",
        "    def model_architecture(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "\n",
        "\n",
        "        for layer in model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "        model.add(GRU(gru_cells))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "        optimiser = tf.keras.optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "conv_gru_tl = ConvGRU_Transfer()\n",
        "conv_gru_tl.initialize_path(project_folder_path)\n",
        "conv_gru_tl.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_gru_tl.initialize_hyperparams(frames_to_sample=20,batch_size=32,num_epochs=25)\n",
        "conv_gru_tl.initialize_modelparams()\n",
        "conv_gru_model_tl=conv_gru_tl.model_architecture(gru_cells=128,dense_neurons=128,dropout=0.5)\n",
        "conv_gru_model_tl.summary()"
      ],
      "metadata": {
        "id": "Gmy_JgWX-T9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl.train_model(conv_gru_model_tl,model_name=\"conv_gru_tl\",model_type=\"CNN_GRU\")\n",
        "conv_gru_tl.plot_model()"
      ],
      "metadata": {
        "id": "H4Jb5oYW-Wz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "The best epoch was the 7th epoch with a validation accuracy of 81% , Categorical accuracy of 88.39% , Validation loss of 0.47, categorical loss of 0.31\n",
        "\n",
        "lets push the accuravy by decreasing batch size & dropout and increasing frames to sample\n",
        "\n",
        "**Model 13: MobileNet+GRU**\n",
        "\n",
        "25 frames to be sampled\n",
        "\n",
        "batch size of 16\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons\n",
        "\n",
        "Epochs = 25\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "gru cells = 128"
      ],
      "metadata": {
        "id": "DxtzQcpF-en8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl2 = ConvGRU_Transfer()\n",
        "conv_gru_tl2.initialize_path(project_folder_path)\n",
        "conv_gru_tl2.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_gru_tl2.initialize_hyperparams(frames_to_sample=25,batch_size=16,num_epochs=25)\n",
        "conv_gru_tl2.initialize_modelparams()\n",
        "conv_gru_model_tl2=conv_gru_tl2.model_architecture(gru_cells=128,dense_neurons=128,dropout=0.25)\n",
        "conv_gru_model_tl2.summary()"
      ],
      "metadata": {
        "id": "T-ol0rfB-mu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl2.train_model(conv_gru_model_tl2,model_name=\"conv_gru_tl2\",model_type=\"CNN_GRU\")\n",
        "conv_gru_tl2.plot_model()"
      ],
      "metadata": {
        "id": "e8bJ9Hiy-qhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "This just increased the overfitting and no improvisation validation loss and accuracy\n",
        "\n",
        "We see loss: 0.1710 - categorical_accuracy: 0.9532 - val_loss: 0.6208 - val_categorical_accuracy: 0.7600\n",
        "\n",
        "Let's increase the batch size to 64 and decrease the frames to sample from 25 to 20 and see if it helps.\n",
        "\n",
        "**Model 14: MobileNet+GRU**\n",
        "\n",
        "20 frames to be sampled\n",
        "\n",
        "batch size of 64\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons\n",
        "\n",
        "Epochs = 25\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "gru cells = 128"
      ],
      "metadata": {
        "id": "y4LEmIAi-uu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl3 = ConvGRU_Transfer()\n",
        "conv_gru_tl3.initialize_path(project_folder_path)\n",
        "conv_gru_tl3.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_gru_tl3.initialize_hyperparams(frames_to_sample=20,batch_size=64,num_epochs=25)\n",
        "conv_gru_tl3.initialize_modelparams()\n",
        "conv_gru_model_tl3=conv_gru_tl3.model_architecture(gru_cells=128,dense_neurons=128,dropout=0.25)\n",
        "conv_gru_model_tl3.summary()"
      ],
      "metadata": {
        "id": "GAzSTTUR-4Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl3.train_model(conv_gru_model_tl3,model_name=\"conv_gru_tl3\",model_type=\"CNN_GRU\")\n",
        "conv_gru_tl3.plot_model()"
      ],
      "metadata": {
        "id": "iWv-MWne-67d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "We see loss: 0.0209 - categorical_accuracy: 1.0000 - val_loss: 0.5939 - val_categorical_accuracy: 0.8100\n",
        "\n",
        "There is a slight decrease in the validation loss from 0.62 to 0.59 and increase in validation accuracy to 81% from 76%\n",
        "\n",
        "Lets reduce the number of parameters by reducing the gru cells and dense neurons to 64 from 128 each\n",
        "\n",
        "**Model 15: MobileNet+GRU**\n",
        "\n",
        "20 frames to be sampled\n",
        "\n",
        "batch size of 64\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "64 dense neurons\n",
        "\n",
        "Epochs = 25\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "gru cells = 64\n",
        "\n",
        "Reducing the number of neurons of gru and dense layer"
      ],
      "metadata": {
        "id": "7tGjN2bm-_gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl4 = ConvGRU_Transfer()\n",
        "conv_gru_tl4.initialize_path(project_folder_path)\n",
        "conv_gru_tl4.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_gru_tl4.initialize_hyperparams(frames_to_sample=20,batch_size=64,num_epochs=25)\n",
        "conv_gru_tl4.initialize_modelparams()\n",
        "conv_gru_model_tl4=conv_gru_tl4.model_architecture(gru_cells=64,dense_neurons=64,dropout=0.25)\n",
        "conv_gru_model_tl4.summary()"
      ],
      "metadata": {
        "id": "Ah58lRcj_LfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl4.train_model(conv_gru_model_tl4,model_name=\"conv_gru_tl4\",model_type=\"CNN_GRU\")\n",
        "conv_gru_tl4.plot_model()"
      ],
      "metadata": {
        "id": "AybXuULP_OKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "We see loss: 0.0319 - categorical_accuracy: 0.9940 - val_loss: 0.5272 - val_categorical_accuracy: 0.8000\n",
        "\n",
        "There is a slight decrease in the validation loss from 0.59 to 0.52\n",
        "\n",
        "lets reduce the batch size to 8 and below configuration with previous increased dense and gru units of 128 , reduce the frames to 15\n",
        "\n",
        "**Model 16: MobileNet+GRU**\n",
        "\n",
        "15 frames to be sampled\n",
        "\n",
        "batch size of 8\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons\n",
        "\n",
        "Epochs = 25\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "gru cells = 128"
      ],
      "metadata": {
        "id": "wYC5kClr_iWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl4_copy = ConvGRU_Transfer()\n",
        "conv_gru_tl4_copy.initialize_path(project_folder_path)\n",
        "conv_gru_tl4_copy.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_gru_tl4_copy.initialize_hyperparams(frames_to_sample=15,batch_size=8,num_epochs=25)\n",
        "conv_gru_tl4_copy.initialize_modelparams()\n",
        "conv_gru_model_tl4_copy=conv_gru_tl4_copy.model_architecture(gru_cells=128,dense_neurons=128,dropout=0.25)\n",
        "conv_gru_model_tl4_copy.summary()"
      ],
      "metadata": {
        "id": "Q9sEzlR7_ri9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl4_copy.train_model(conv_gru_model_tl4_copy,model_name=\"conv_gru_tl4_copy\",model_type=\"CNN_GRU"
      ],
      "metadata": {
        "id": "6NdXC5yG_ygm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl4_copy.plot_model()"
      ],
      "metadata": {
        "id": "O7ruk_Ip_z5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "We see loss: 0.1109 - categorical_accuracy: 0.9698 - val_loss: 0.5595 - val_categorical_accuracy: 0.8100\n",
        "\n",
        "There is a slight increase in the validation loss from 0.52 to 0.56\n",
        "Reduced overfitting from 99:80 to 96:81 , train:val accuracy.\n",
        "\n",
        "Lets try training on whole mobilenet model with the same parameters\n",
        "\n",
        "**Model 17: MobileNet(on all the layers)+GRU**\n",
        "\n",
        "15 frames to be sampled\n",
        "\n",
        "batch size of 8\n",
        "\n",
        "Images cropped to 120,120\n",
        "\n",
        "128 dense neurons\n",
        "\n",
        "Epochs = 25\n",
        "\n",
        "dropout = 0.25\n",
        "\n",
        "gru cells = 128"
      ],
      "metadata": {
        "id": "TQjuzmjx_4Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
        "\n",
        "class ConvGRU_Transfer_NoWeights(ModelClass):\n",
        "\n",
        "    def model_architecture(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "\n",
        "\n",
        "        for layer in model.layers:\n",
        "            layer.trainable = True\n",
        "\n",
        "\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "        model.add(GRU(gru_cells))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "        optimiser = tf.keras.optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model\n",
        "\n",
        "conv_gru_tl4_now = ConvGRU_Transfer_NoWeights()\n",
        "conv_gru_tl4_now.initialize_path(project_folder_path)\n",
        "conv_gru_tl4_now.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_gru_tl4_now.initialize_hyperparams(frames_to_sample=15,batch_size=8,num_epochs=25)\n",
        "conv_gru_tl4_now.initialize_modelparams()\n",
        "conv_gru_model_tl4_now=conv_gru_tl4_now.model_architecture(gru_cells=128,dense_neurons=128,dropout=0.25)\n",
        "conv_gru_model_tl4_now.summary()"
      ],
      "metadata": {
        "id": "z6JgqjwQAA3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl4_now.train_model(conv_gru_model_tl4_now,model_name=\"conv_gru_tl4_now\",model_type=\"CNN_GRU\")"
      ],
      "metadata": {
        "id": "qyWak89AAECk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gru_tl4_now.plot_model()"
      ],
      "metadata": {
        "id": "Lnxt7496ALUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Thoughts:**\n",
        "\n",
        "loss: 0.0216 - categorical_accuracy: 0.9940 - val_loss: 0.1122 - val_categorical_accuracy: 0.9400\n",
        "\n",
        "This is the best model till now with the highest validation accuracy of 94% and least validation loss of just 0.1122\n",
        "\n",
        "**Which Model to Choose?? We have got a tradeoff between two models:**\n",
        "\n",
        "Model3 -> Conv3D -> Epoch25th Model -> Validation accuracy 86% -> validation loss 0.46 -> Model Size 14MB -> Trainable Params 11,55,397\n",
        "\n",
        "Model17 -> MobileNet(with complete training over all the layers) + GRU -> epoch 22nd Model -> Validation accuracy 94% -> validation loss 0.1122 -> Model Size 44MB -> 36,93,253\n",
        "\n",
        "**We select the second one which is the MobileNet+GRU for the following reasons:**\n",
        "\n",
        "The validation loss is way lesser ~0.1122 than the conv3D Model with 0.46\n",
        "\n",
        "The accuracy is 94% which is remarkable than Conv3D model with 86%\n",
        "\n",
        "Trainable params are almost thrice but a 44MB model is not a huge model when compared to CNNs which end up in GBs.\n",
        "\n",
        "Models in MBs are easily deployable on smart devices."
      ],
      "metadata": {
        "id": "gIPPsRIwAQv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing the model on a random batch in validation set**"
      ],
      "metadata": {
        "id": "M22oNpspAc71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model\n",
        "import time\n",
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/conv_gru_tl4_now_2021-10-2617_37_49.157949/Mobilenet_NoW_mode"
      ],
      "metadata": {
        "id": "h84wUr11AZwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator=ConvGRU_Transfer_NoWeights()\n",
        "test_generator.initialize_path(project_folder_path)\n",
        "test_generator.initialize_image_properties(image_height=120,image_width=120)\n",
        "test_generator.initialize_hyperparams(frames_to_sample=15,batch_size=8,num_epochs=25)\n",
        "\n",
        "g=test_generator.generator(test_generator.val_path,test_generator.val_doc,model_type=\"Conv_GRU\")\n",
        "batch_data, batch_labels=next(g)"
      ],
      "metadata": {
        "id": "zsKjgpb-An1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_labels. ##true labels"
      ],
      "metadata": {
        "id": "Q_52cEhWArU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(model.predict(batch_data[:,:,:,:,:]),axis=1)) ## predicted labels"
      ],
      "metadata": {
        "id": "jXs8kuE2AtmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "for video in os.listdir('/content/drive/MyDrive/Videosforgesturerecognition'):\n",
        "  capture = cv2.VideoCapture('/content/drive/MyDrive/Videosforgesturerecognition/'+video)\n",
        "\n",
        "  frameNr = 0\n",
        "  if video.split(\".\")[0] not in os.listdir('/content/drive/MyDrive/GR_FramedVideos'):\n",
        "    os.mkdir('/content/drive/MyDrive/GR_FramedVideos/'+video.split(\".\")[0])\n",
        "  while (True):\n",
        "\n",
        "      success, frame = capture.read()\n",
        "\n",
        "      if success:\n",
        "          frame = cv2.resize(frame,(120,120))\n",
        "          frame = cv2.rotate(frame, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "          cv2.imwrite('/content/drive/MyDrive/GR_FramedVideos/'+video.split(\".\")[0]+'/frame_'+str(frameNr)+'.jpg', frame)\n",
        "\n",
        "      else:\n",
        "          break\n",
        "\n",
        "      frameNr = frameNr+1\n",
        "\n",
        "  capture.release()"
      ],
      "metadata": {
        "id": "RpATq2IIAu5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##modifying the generator to accomodate our own video with a batch size of 1 as we want to test on our own video\n",
        "\n",
        "def test_generator(source_path, folder_list, model_type = \"Conv3D\",frames_to_sample=15,image_height=120,image_width=120,batch_size=1,channels=3,num_classes=5):\n",
        "      if model_type == \"Conv3D\":\n",
        "        img_idx = [rn.randint(0, total_frames-1) for i in range(frames_to_sample)] #create a list of image numbers you want to use for a particular video\n",
        "      else:\n",
        "        img_idx = [i for i in range(frames_to_sample)] ## CNN+GRU\n",
        "      # print(img_idx)\n",
        "      while True:\n",
        "          t = np.random.permutation(folder_list)\n",
        "          num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
        "          for batch_no in range(num_batches): # we iterate over the number of batches\n",
        "              batch_data = np.zeros((batch_size,len(img_idx),image_height,image_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "              batch_labels = np.zeros((batch_size,num_classes)) # batch_labels is the one hot representation of the output\n",
        "              for folder in range(batch_size): # iterate over the batch_size\n",
        "                  imgs = os.listdir(source_path+'/'+ t[folder + (batch_no*batch_size)].split(',')[0]) # read all the images in the folder\n",
        "                  # print(\"imgs: \",imgs)\n",
        "                  for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                      image = imread(source_path+'/'+ t[folder + (batch_no*batch_size)].strip().split(',')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                      if image.shape == (360,360,3): ###if images are of 360 x 360\n",
        "                        image = imresize(image,(image_height,image_width,channels))\n",
        "                      else:\n",
        "                        image = image[:,(image.shape[0] - image_height ) // 2 : image.shape[0] - (image.shape[0] - image_height ) // 2] ### if lower quality just centre crop\n",
        "                      #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                      #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "\n",
        "\n",
        "                      batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #normalise and feed in the image\n",
        "                      batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0 #normalise and feed in the image\n",
        "                      batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0 #normalise and feed in the image\n",
        "\n",
        "                  batch_labels[folder, int(t[folder + (batch_no*batch_size)].strip().split(',')[2])] = 1\n",
        "              yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "HjfXBsqJA07m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = test_generator(\n",
        "    source_path = \"/content/drive/MyDrive/GR_FramedVideos\",\n",
        "    folder_list = np.random.permutation(open(\"/content/drive/MyDrive/test.csv\").readlines()),\n",
        "    model_type = \"CNN_GRU\",\n",
        "    frames_to_sample=15\n",
        ")"
      ],
      "metadata": {
        "id": "h37F1zYmA2Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##generate a video representation\n",
        "batch_data, batch_labels=next(test_gen)"
      ],
      "metadata": {
        "id": "BCjZHL5xA4cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_labels ##true label is one hot at index 3"
      ],
      "metadata": {
        "id": "ROCSGHL5A8Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(model.predict(batch_data[:,:,:,:,:]),axis=1)) ## predicted labels"
      ],
      "metadata": {
        "id": "y9WhYvXgA-Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## lets look at an image from this video\n",
        "plt.imshow(batch_data[0][13])\n",
        ""
      ],
      "metadata": {
        "id": "CM2Q_Nx9A_IX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}